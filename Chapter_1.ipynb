{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Reinforcement learning\n",
    "\n",
    "- Deep RL is a type of machine learning where an agent learns **how to behave** in an environment **by performing actions** and **observing the results**.\n",
    " \n",
    "- Overall idea is how an agent will learn from the environment by **interacting** with it and **recieve rewards** as feedback for performing actions.\n",
    "\n",
    "- A formal definition, \"Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Framework\n",
    "\n",
    "#### The RL Process\n",
    "\n",
    "![Image from Gyazo](https://i.gyazo.com/11a118b9e88e028726ef37b3a1014bb7.png)\n",
    "\n",
    "- Agent recieves **State S0** from the Environment -- we recieve the first frame of our game (Environment)\n",
    "- Based on **state S0**, Agent takes **action A0** -- our agent moves forward\n",
    "- Environment goes to a new **state S1** -- new frame\n",
    "- Environment gives some **reward R1** to the Agent, (+1 reward for not being dead)\n",
    "\n",
    "An overall RL loop outputs a sequence of **state, action, reward and next state**.  (S0, A0, R1, S1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Hypothesis\n",
    "\n",
    "- RL is based on reward hypothesis, which can be defined as **maximization of the expected return (cumulative reward)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Property\n",
    "- Markov Decision Process\n",
    "- Markov process in the future **depends only on the present state and does not depend on the past history**\n",
    "- Does not remember the past if the present state is given\n",
    "- Our agent needs **only the current state to decide** what action to take and **not the history of all the states and actions** they took before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation/States Space\n",
    "- The information our agent gets from the environment.\n",
    "- **State** is a complete description of the state of the world (no hidden information)\n",
    "- **Observation** is a partial description of the state. (eg, current frame of a mario game level, ie, cannot see the full map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space\n",
    "- Set of all **possible actions in an environment**\n",
    "- **Discrete** space - number of possible actions is **finite**(eg, mario has 4 directions and a jump)\n",
    "- **Continuous** space - number of possible actions is **infinite?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards and discounting\n",
    "- Reward is usually the only feedback for the agent\n",
    "- Decides if the action taken by agent was good or not\n",
    "- Cumulative reward can be written as:\n",
    "\n",
    "[![Image from Gyazo](https://i.gyazo.com/96182f6f7cf80982158925baaa5ab99a.png)](https://gyazo.com/96182f6f7cf80982158925baaa5ab99a)\n",
    "\n",
    "- However, we cannot just add all rewards like this.\n",
    "- Rewards that come sooner(at the beginnning of the game) **are more likely to happen** since they are more predictable than the long-term future reward\n",
    "\n",
    "##### Discounting\n",
    "- We define a discount rate called gamma. **Between 0 and 1** (0.95<x<0.99)\n",
    "- Larger the gamma, smaller the discount. This means our agent **cares more about the long-term reward**.\n",
    "- Smaller the gamma, bigger the discount. This means our agent **cares more about the short term reward**\n",
    "----\n",
    "- Each reward will be discounted by gamma to the **exponent of the time step**. As time step increases, **the future reward is less and less likely to happen**.\n",
    "[![Image from Gyazo](https://i.gyazo.com/308e136e34e4c3a0e90b0d28f5dd20ce.png)](https://gyazo.com/308e136e34e4c3a0e90b0d28f5dd20ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of Tasks\n",
    "- **Episodic Task** - Has a starting point and an ending point (**a terminal state**) - This creates an **episode**: a list of states, actions, rewards and new states. (eg, in mario level, episode begin at the launch of new mario level, and ends when you're killed or reach the end of the level).\n",
    "- **Continuous tasks** - Tasks continue forever (no terminal state).\n",
    "- Here, the agent must learn **how to choose the best actions and simultaneously interact with the environment** (eg, automated stock trading, no start point or terminal state. Agent runs until stopped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration / Exploitation tradeoff\n",
    "- Exploration is exploring the environment by trying random actions to **find more information about the environment**\n",
    "- Exploitation is **exploiting known information to maximize the reward**.\n",
    "\"\\\n",
    "\\ \"\n",
    "- The goal of our RL agent is to maximize the expected cumulative reward. However, this can lead to a **common trap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy example for this tradeoff (Choice of restuarant)\n",
    "- *Exploitation* - You go everyday to the same restuarant that you know is good and **take the risk to miss another restuarant**\n",
    "- *Exploration* - You try restuarants you never went to before, with the risk of having a bad experience **but the probable opportunity of a fantastic experience**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploration - Try random actions in order to find more information about the environment\n",
    "- Exploitation - Using known information to maximize the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two approaches for solving RL problems\n",
    "- Build an RL agent that can select the actions that maximize its expected cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy π: Agent's brain\n",
    "- The function that tells us what action to take given the current state we are in.\n",
    "[![Image from Gyazo](https://i.gyazo.com/9b2a0f4311df59da5c231171e0388e4e.png)](https://gyazo.com/9b2a0f4311df59da5c231171e0388e4e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the function we want to learn, to find the optimal Policy.\n",
    "- The function that maximizes expected return when the agent acts according to it.\n",
    "- Is found through training.\n",
    "    - **Policy based method** - Directly, by teaching the agent to learn which action to take, given the state is in.\n",
    "    - **Value based method** - Indirectly, teach the agent to learn which state is more valuable, and then take the action that leads to more valuable states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy-Based Methods\n",
    "- We learn a policy function direclty.\n",
    "- This function will map from each state to the best corresponding action at that state.\n",
    "- **Deterministic Policy** - A policy at a given state **will always return the same action**\n",
    "- **Stochastic Policy** - Outputs a **pobability distribution over actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Image from Gyazo](https://i.gyazo.com/b5bfec449bfcdbe3d53752676c4ad6e6.png)](https://gyazo.com/b5bfec449bfcdbe3d53752676c4ad6e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value-based methods\n",
    "- Here, instead of training a policy function, we **train a value function** that maps a state to the expected value **of being at that state**.\n",
    "- The value of a state is the **expected discounted return** the agent can get if it starts in that state, and then acts according to our policy (going to the state with the highest value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Image from Gyazo](https://i.gyazo.com/7b917712a25ec30e2fa45c2ade3b078f.png)](https://gyazo.com/7b917712a25ec30e2fa45c2ade3b078f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Image from Gyazo](https://i.gyazo.com/ab228fb56580a2b2fb1170eb2dc15f4d.png)](https://gyazo.com/ab228fb56580a2b2fb1170eb2dc15f4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep - Reinforcement learning\n",
    "- Q-Learning (classical Reinforcement Learning) (Value based) (traditional algorithm to create Q table)\n",
    "- Deep Q-learning (Value based) (Use a NN to approximate q value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided summary\n",
    "- Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.\n",
    "\n",
    "- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.\n",
    "\n",
    "- The RL process is a loop that outputs a sequence of state, action, reward and next state.\n",
    "\n",
    "- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.\n",
    "\n",
    "- To solve an RL problem, you want to find an optimal policy, the policy is the “brain” of your AI that will tell us what action to take given a state. The optimal one is the one who gives you the actions that max the expected return.\n",
    "\n",
    "- There are two ways to find your optimal policy:\n",
    "    - By training your policy directly: policy-based methods.\n",
    "    - By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n",
    "- Finally, we speak about Deep RL because we introduces deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name “deep.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baseline 3\n",
    "[![Image from Gyazo](https://i.gyazo.com/5871a3ca098c9d9e7a79e1e9d015f222.png)](https://gyazo.com/5871a3ca098c9d9e7a79e1e9d015f222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
