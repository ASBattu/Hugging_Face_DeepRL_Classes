{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Published model can be found at https://huggingface.co/Battu007/V4_PPO2_LunarLander_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose environment name\n",
    "env_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/huggingface/deep-rl-class\n",
    "- https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard\n",
    "- https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run env for X steps to check it out\n",
    "\n",
    "#Create a new environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#Reset environment\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(150):\n",
    "    env.render()           #Render env for every step\n",
    "    time.sleep(0.01)       #Slow down the simulation to view\n",
    "    action = env.action_space.sample()     #Give a random action\n",
    "\n",
    "    #Get back observation, reward,, simulation end flag and info\n",
    "    observation, reward, done, info = env.step(action)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LunarLander-v2 Action Space:  Discrete(4)\n",
      "LunarLander-v2 Observation Space  Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env_name , \"Action Space: \", env.action_space)\n",
    "print(env_name, \"Observation Space \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Action Space\n",
    "    - There are four discrete actions available: do nothing, fire left\n",
    "    orientation engine, fire main engine, fire right orientation engine.\n",
    "- Observation Space\n",
    "    - There are 8 states: the coordinates of the lander in `x` & `y`, its linear\n",
    "    velocities in `x` & `y`, its angle, its angular velocity, and two booleans\n",
    "    that represent whether each leg is in contact with the ground or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing model\n",
    "- From SB3 implementations, possible models include:\n",
    "    - ARS, A2C, DQN, PPO, TRPO (Chosen to test out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO Implementation (MLP Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_PPO = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on env\n",
    "model_PPO.learn(total_timesteps = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\amito\\anaconda3\\envs\\GYM_cpu\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=226.69 +/- 25.86449951482193\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model\n",
    "eval_env = gym.make(env_name)\n",
    "mean_reward, std_reward = evaluate_policy(model_PPO, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PPO.save(\"PPO_500K_Lunar_Lander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode:  1\n",
      "Current episode:  2\n",
      "Current episode:  3\n",
      "Current episode:  4\n",
      "Current episode:  5\n",
      "Current episode:  6\n",
      "Current episode:  7\n",
      "Current episode:  8\n",
      "Current episode:  9\n",
      "Current episode:  10\n"
     ]
    }
   ],
   "source": [
    "#Run trained agent for X episodes to view performance\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    dones = False\n",
    "    print(\"Current episode: \", ep+1)\n",
    "    while not dones:\n",
    "        action, _states = model_PPO.predict(obs, deterministic=True)\n",
    "        obs, reward, dones, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2C Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_A2C = A2C(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 5,\n",
    "    gamma = 0.99,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    learning_rate = 0.001,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on env\n",
    "model_A2C.learn(total_timesteps = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\amito\\anaconda3\\envs\\GYM_cpu\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-12.14 +/- 71.02340042939036\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model\n",
    "eval_env = gym.make(env_name)\n",
    "mean_reward, std_reward = evaluate_policy(model_A2C, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode:  1\n",
      "Current episode:  2\n",
      "Current episode:  3\n",
      "Current episode:  4\n",
      "Current episode:  5\n"
     ]
    }
   ],
   "source": [
    "#Run trained agent for X episodes to view performance\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    dones = False\n",
    "    print(\"Current episode: \", ep+1)\n",
    "    while not dones:\n",
    "        action, _states = model_A2C.predict(obs, deterministic=True)\n",
    "        obs, reward, dones, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A2C.save(\"A2C_500K_Lunar_Lander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_DQN = DQN(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on env\n",
    "model_DQN.learn(total_timesteps = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\amito\\anaconda3\\envs\\GYM_cpu\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-41.80 +/- 71.52575431835412\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model\n",
    "eval_env = gym.make(env_name)\n",
    "mean_reward, std_reward = evaluate_policy(model_DQN, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode:  1\n",
      "Current episode:  2\n",
      "Current episode:  3\n",
      "Current episode:  4\n",
      "Current episode:  5\n"
     ]
    }
   ],
   "source": [
    "#Run trained agent for X episodes to view performance\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    dones = False\n",
    "    print(\"Current episode: \", ep+1)\n",
    "    while not dones:\n",
    "        action, _states = model_DQN.predict(obs, deterministic=True)\n",
    "        obs, reward, dones, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DQN.save(\"DQN_500K_Lunar_Lander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO 2 different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_PPO2 = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 32,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on env\n",
    "model_PPO2.learn(total_timesteps = 2e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=275.96 +/- 21.646267077384305\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_name)])\n",
    "\n",
    "# eval_env = gym.make(env_name)\n",
    "mean_reward, std_reward = evaluate_policy(model_PPO2, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode:  1\n",
      "Current episode:  2\n",
      "Current episode:  3\n",
      "Current episode:  4\n",
      "Current episode:  5\n"
     ]
    }
   ],
   "source": [
    "#Run trained agent for X episodes to view performance\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    dones = False\n",
    "    print(\"Current episode: \", ep+1)\n",
    "    while not dones:\n",
    "        action, _states = model_PPO2.predict(obs, deterministic=True)\n",
    "        obs, reward, dones, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model_PPO2.save(\"PPO2_5M_Lunar_Lander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "#Loading Previous saved model\n",
    "model_PPO2 = PPO.load(\"PPO2_5M_Lunar_Lander\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = env_name\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "repo_id = \"Battu007/'___'\"\n",
    "\n",
    "commit_message = \"PPO Hyperparemeter tune 2M steps LL-2 agent -- Wrapped in DummyVecEnv\"\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_name)])\n",
    "\n",
    "package_to_hub(model=model_PPO2, # Our trained model\n",
    "               model_name=\"V4_PPO_LL\", # The name of our trained model \n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message,\n",
    "               token = '')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae1db68e3dfa5a11c986f24b3b2047efd58325489817861191ec752dcebd55c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GYM_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
